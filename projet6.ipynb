{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 6 - Layerwise Relevance Propagation pour réseaux de neurones graphiques\n",
    "\n",
    "Elyes KHALFALLAH & Edouard CHAPPON\n",
    "\n",
    "MALIA\n",
    "\n",
    "---\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-igraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def relevance_curves_xy_computation(nodes_x_coors_list: list, nodes_y_coors_list: list):\n",
    "    \"\"\"\n",
    "    This method performs the computation of x,y coordinate points for the relevance curves (or equivalently\n",
    "    curved lines) inside the plot. The relevance curves are curves that depict the relevance of a walk,\n",
    "    comprised of 3 nodes (and 2 edges connecting them) by their colour. Since several nodes can be part of\n",
    "    many different walks, it was decided that curved lines, surrounding (\"around\") the graphs will show the\n",
    "    relevances of each walk in the graph (which is depicted in black) - so that there is no overlap.\n",
    "    This is a plot helper method and not a relevance computation method.\n",
    "\n",
    "    :param nodes_x_coors_array: List of the 3 x-coordinates of the nodes in the walk\n",
    "    :param nodes_y_coors_array: List of the 3 y-coordinates of the nodes in the walk\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    relevance_line_curve_x_coors_list = np.array(nodes_x_coors_list)\n",
    "    relevance_line_curve_y_coors_list = np.array(nodes_y_coors_list)\n",
    "\n",
    "    if (\n",
    "        relevance_line_curve_x_coors_list[0]\n",
    "        == relevance_line_curve_x_coors_list[1]\n",
    "        == relevance_line_curve_x_coors_list[2]\n",
    "        and relevance_line_curve_y_coors_list[0]\n",
    "        == relevance_line_curve_y_coors_list[1]\n",
    "        == relevance_line_curve_y_coors_list[2]\n",
    "    ):\n",
    "        relevance_line_curve_x_coors_list = relevance_line_curve_x_coors_list[\n",
    "            0\n",
    "        ] + 0.1 * np.cos(np.linspace(0, 2 * np.pi, 50))\n",
    "        relevance_line_curve_y_coors_list = relevance_line_curve_y_coors_list[\n",
    "            0\n",
    "        ] + 0.1 * np.sin(np.linspace(0, 2 * np.pi, 50))\n",
    "    else:\n",
    "        relevance_line_curve_x_coors_list = (\n",
    "            0.75 * relevance_line_curve_x_coors_list\n",
    "            + 0.25 * relevance_line_curve_x_coors_list.mean()\n",
    "        )\n",
    "        relevance_line_curve_y_coors_list = (\n",
    "            0.75 * relevance_line_curve_y_coors_list\n",
    "            + 0.25 * relevance_line_curve_y_coors_list.mean()\n",
    "        )\n",
    "\n",
    "        relevance_line_curve_x_coors_list = np.concatenate(\n",
    "            [\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_x_coors_list[0],\n",
    "                    relevance_line_curve_x_coors_list[0],\n",
    "                    41,\n",
    "                ),\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_x_coors_list[0],\n",
    "                    relevance_line_curve_x_coors_list[1],\n",
    "                    20,\n",
    "                ),\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_x_coors_list[1],\n",
    "                    relevance_line_curve_x_coors_list[2],\n",
    "                    20,\n",
    "                ),\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_x_coors_list[2],\n",
    "                    relevance_line_curve_x_coors_list[2],\n",
    "                    41,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        relevance_line_curve_y_coors_list = np.concatenate(\n",
    "            [\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_y_coors_list[0],\n",
    "                    relevance_line_curve_y_coors_list[0],\n",
    "                    41,\n",
    "                ),\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_y_coors_list[0],\n",
    "                    relevance_line_curve_y_coors_list[1],\n",
    "                    20,\n",
    "                ),\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_y_coors_list[1],\n",
    "                    relevance_line_curve_y_coors_list[2],\n",
    "                    20,\n",
    "                ),\n",
    "                np.linspace(\n",
    "                    relevance_line_curve_y_coors_list[2],\n",
    "                    relevance_line_curve_y_coors_list[2],\n",
    "                    41,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        filt = np.exp(-np.linspace(-2, 2, 41) ** 2)\n",
    "        filt = filt / filt.sum()\n",
    "\n",
    "        relevance_line_curve_x_coors_list = np.convolve(\n",
    "            relevance_line_curve_x_coors_list, filt, mode=\"valid\"\n",
    "        )\n",
    "        relevance_line_curve_y_coors_list = np.convolve(\n",
    "            relevance_line_curve_y_coors_list, filt, mode=\"valid\"\n",
    "        )\n",
    "\n",
    "    return relevance_line_curve_x_coors_list, relevance_line_curve_y_coors_list\n",
    "\n",
    "\n",
    "def compute_walks(adj_matrix: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute all walks of length 3\n",
    "\n",
    "    :param adj_matrix: Adjacency matrix of graph\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    w = []\n",
    "\n",
    "    for v1 in np.arange(len(adj_matrix)):\n",
    "        for v2 in np.where(adj_matrix[v1])[0]:\n",
    "            for v3 in np.where(adj_matrix[v2])[0]:\n",
    "\n",
    "                w += [(v1, v2, v3)]\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def set_graph_layout(adj_matrix: np.ndarray, seed):\n",
    "    \"\"\"\n",
    "    Creates a graph and sets the graph layout according to a set of edges and nodes in adj_matrix\n",
    "\n",
    "    :param adj_matrix: Adjacency matrix\n",
    "    :param seed: Random seed of graph\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    graph = igraph.Graph()\n",
    "    graph.add_vertices(len(adj_matrix))\n",
    "    graph.add_edges(zip(*np.where(adj_matrix == 1)))\n",
    "\n",
    "    return np.array(list(graph.layout_kamada_kawai()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "import igraph\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# [A.] Create example graphs and plots thereof =========================================================================\n",
    "########################################################################################################################\n",
    "def create_scale_free_graph(\n",
    "    nodes_nr: int, input_seed: int = 0, embed: bool = False, growth=None\n",
    "):\n",
    "    \"\"\"\n",
    "    # Imlementation of the Barabasi-Albert model for the creation of scale-free graphs.\n",
    "    A scale-free graph has a degree distribution that follows a power law - asymptotically.\n",
    "    The fraction P(k) of nodes in the network having k connections to other nodes goes for large values of k:\n",
    "    P(k) ∼ pow(k, −γ)\n",
    "    In this method the γ (gamma) is sampled uniformly being either 1, 2, or 3.\n",
    "\n",
    "    :param nodes_nr: Number of graph nodes\n",
    "    :param input_seed: Seed for the random generation\n",
    "    :param embed: Parameter that will be used for visualization\n",
    "    :param growth: Growth parameter of the Barabasi-Albert (BA) method. The scale-free graph increases over time.\n",
    "    At each timestep a new node with \"growth\" number of links that connect the new node to \"growth\" number of nodes\n",
    "    already in the network is added.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # [1.] Define number of nodes and the adjacency matrix -------------------------------------------------------------\n",
    "    adj_matrix = np.zeros([nodes_nr, nodes_nr])  # Adjacency Matrix\n",
    "    adj_matrix[1, 0] = 1\n",
    "    adj_matrix[0, 1] = 1\n",
    "\n",
    "    # The generated graph that increases over time. At each timestep a new node with \"growth\" number of links that -----\n",
    "    # connect the new node to \"growth\" number of nodes already in the network is added ---------------------------------\n",
    "    random = np.random.mtrand.RandomState(input_seed)\n",
    "    growth = growth if growth is not None else random.randint(1, 3)\n",
    "    N0 = 2\n",
    "\n",
    "    # [2.] Create the edges according to the growth parameter ----------------------------------------------------------\n",
    "    #      The number of nodes in the graph is specified here. What is not specified is the edges.\n",
    "    #      The code implementation defines the process of creating the scale-free graph from scratch.\n",
    "    #      Assuming that we are in a step of this process, and we already have a graph of some sort,\n",
    "    #      we can think of incrementing this graph by inserting a new node, that will be connected to\n",
    "    #      some other in the already existing graph. The decision of which node will the new one \"dock to\"\n",
    "    #      (have an edge with) is specified by the growth factor. It defines with what probability the\n",
    "    #      new node will be connected to the nodes the graph already has.\n",
    "    for i in range(N0, nodes_nr):\n",
    "        if growth == 1:\n",
    "            tt = 1  # Barabasi-Albert with growth 1\n",
    "        elif growth == 2:\n",
    "            tt = 2  # Barabasi-Albert with growth 2\n",
    "        else:\n",
    "            tt = 1 + 1 * ((growth - 1) > random.uniform(0, 1))\n",
    "\n",
    "        p = adj_matrix.sum(axis=0) / adj_matrix.sum()\n",
    "\n",
    "        for j in random.choice(nodes_nr, tt, p=p, replace=False):\n",
    "            adj_matrix[i, j] = 1\n",
    "            adj_matrix[j, i] = 1\n",
    "\n",
    "    r = random.permutation(len(adj_matrix))\n",
    "    adj_matrix = adj_matrix[r][:, r] * 1.0\n",
    "\n",
    "    # [3.] Add Self-Connections in the Adjacency matrix ----------------------------------------------------------------\n",
    "    adj_matrix = adj_matrix + np.identity(len(adj_matrix))\n",
    "\n",
    "    # [4.] Compute Laplacian of the graph ------------------------------------------------------------------------------\n",
    "    D = adj_matrix.sum(axis=1)\n",
    "    laplacian_matrix = torch.FloatTensor(adj_matrix / (np.outer(D, D) ** 0.5 + 1e-9))\n",
    "\n",
    "    # [5.] Compute the graph information data structure (dictionary) ---------------------------------------------------\n",
    "    #      The dataset is comprised by pairs of (graph_of_growth_factor, growth_factor)\n",
    "    #      the second one being the target, as one sees in the returned data structure\n",
    "    return {\n",
    "        \"adjacency\": torch.FloatTensor(adj_matrix),\n",
    "        \"laplacian\": laplacian_matrix,\n",
    "        \"target\": growth,\n",
    "        \"layout\": set_graph_layout(adj_matrix, input_seed) if embed else None,\n",
    "        \"walks\": compute_walks(adj_matrix),\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_graph(graph_data_structure: dict, ax):\n",
    "    \"\"\"\n",
    "    Function to visualise a graph\n",
    "\n",
    "    :param graph_data_structure: Graph information data structure (dictionary)\n",
    "    :param ax: Axis of plot\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # [1.] Arrange graph layout for plot -------------------------------------------------------------------------------\n",
    "    r = graph_data_structure[\"layout\"]\n",
    "    r = r - r.min(axis=0)\n",
    "    r = r / r.max(axis=0) * 2 - 1\n",
    "\n",
    "    # [2.] Plot the edges ----------------------------------------------------------------------------------------------\n",
    "    nodes_nr = len(graph_data_structure[\"adjacency\"])\n",
    "    for i in np.arange(nodes_nr):\n",
    "        for j in np.arange(nodes_nr):\n",
    "            if graph_data_structure[\"adjacency\"][i, j] > 0 and i != j:\n",
    "                plt.plot(\n",
    "                    [r[i, 0], r[j, 0]],\n",
    "                    [r[i, 1], r[j, 1]],\n",
    "                    color=\"gray\",\n",
    "                    lw=0.5,\n",
    "                    ls=\"dotted\",\n",
    "                )\n",
    "\n",
    "    # [3.] Plot the nodes ----------------------------------------------------------------------------------------------\n",
    "    ax.plot(r[:, 0], r[:, 1], \"o\", color=\"black\", ms=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some example graphs ---------------------------------------------------------------------------------------------\n",
    "sample_ids = [1, 3, 4, 5]\n",
    "nodes_nr = 10\n",
    "\n",
    "plt.figure(figsize=(3 * len(sample_ids), 3))\n",
    "for ids, seed in enumerate(sample_ids):\n",
    "\n",
    "    ax = plt.subplot(1, len(sample_ids), ids + 1)\n",
    "    example_graph = create_scale_free_graph(nodes_nr, input_seed=seed, embed=True)\n",
    "\n",
    "    visualize_graph(example_graph, ax=ax)\n",
    "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.xlim(-1.2, 1.2)\n",
    "    plt.ylim(-1.2, 1.2)\n",
    "    ax.set_title(f\"growth={example_graph['target']}\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# [B.] Create GraphNet GNN ::: Define architecture, forward and lrp methods ============================================\n",
    "########################################################################################################################\n",
    "class GraphNet:\n",
    "    \"\"\"\n",
    "    Graph Net - The Graph Neural Network (GNN)\n",
    "\n",
    "    The adjacency matrix is not enough in general. Graph nodes do have features\n",
    "    (think of values representing size, color and so on) and edges also having\n",
    "    corresponding ones (distance, weight...). So only in this case, this very\n",
    "    simple graph where in principle only connectivity is used for the\n",
    "    classification, the adjacency matrix is enough.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_layer_size: int, hidden_layer_size: int, output_layer_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        GNN init architecture\n",
    "\n",
    "        :param input_layer_size: Input layer size\n",
    "        :param hidden_layer_size: Hidden layer size\n",
    "        :param output_layer_size: Output layer size\n",
    "        \"\"\"\n",
    "\n",
    "        # [1.] Architecture of the GNN ---------------------------------------------------------------------------------\n",
    "        #      Weights/parameters of the GNN ---------------------------------------------------------------------------\n",
    "        #      U the weights/parameters of the input layer, V the weights parameters of the last layer -----------------\n",
    "        self.U = torch.nn.Parameter(\n",
    "            torch.FloatTensor(\n",
    "                np.random.normal(\n",
    "                    0, input_layer_size**-0.5, [input_layer_size, hidden_layer_size]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.W1 = torch.nn.Parameter(\n",
    "            torch.FloatTensor(\n",
    "                np.random.normal(\n",
    "                    0, hidden_layer_size**-0.5, [hidden_layer_size, hidden_layer_size]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.W2 = torch.nn.Parameter(\n",
    "            torch.FloatTensor(\n",
    "                np.random.normal(\n",
    "                    0, hidden_layer_size**-0.5, [hidden_layer_size, hidden_layer_size]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.V = torch.nn.Parameter(\n",
    "            torch.FloatTensor(\n",
    "                np.random.normal(\n",
    "                    0, hidden_layer_size**-0.5, [hidden_layer_size, output_layer_size]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # [2.] Weights/parameters of the GNN ---------------------------------------------------------------------------\n",
    "        self.params = [self.U, self.W1, self.W2, self.V]\n",
    "\n",
    "    def forward_pass(self, adj_matrix: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the GNN\n",
    "\n",
    "        :param adj_matrix: Adjacency matrix of the GNN\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        H = torch.eye(len(adj_matrix))\n",
    "        H = H.matmul(self.U).clamp(min=0)\n",
    "        H = (adj_matrix.transpose(1, 0).matmul(H.matmul(self.W1))).clamp(min=0)\n",
    "        H = (adj_matrix.transpose(1, 0).matmul(H.matmul(self.W2))).clamp(min=0)\n",
    "        H = H.matmul(self.V).clamp(min=0)\n",
    "\n",
    "        return H.mean(dim=0)\n",
    "\n",
    "    def lrp_computation(\n",
    "        self, adj_matrix: torch.Tensor, gamma: float, target: int, indexes: tuple\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the LRP relevance, as presented in the original paper: https://arxiv.org/abs/2006.03589\n",
    "\n",
    "        :param adj_matrix: Adjacency matrix of the graph\n",
    "        :param gamma: LRP gamma hyperparameter\n",
    "        :param target: Target class\n",
    "        :param indexes: Indexes of nodes in the graph\n",
    "        \"\"\"\n",
    "\n",
    "        # [1.] Create a column array which contains only one non-zero element ------------------------------------------\n",
    "        #      in the position of the node index contained in the walk -------------------------------------------------\n",
    "        if indexes is not None:\n",
    "\n",
    "            j, k = indexes\n",
    "            M_j = torch.FloatTensor(np.eye(len(adj_matrix))[j][:, np.newaxis])\n",
    "            M_k = torch.FloatTensor(np.eye(len(adj_matrix))[k][:, np.newaxis])\n",
    "\n",
    "        # [2.] Weights of the GNN (U, W1, W2, V) plus a weighted version of their positive part ------------------------\n",
    "        #      The ReLu of equation 9 in page 5, Table 1 of the original paper is implemented by the clamp() method ----\n",
    "        W1p = self.W1 + gamma * self.W1.clamp(min=0)\n",
    "        W2p = self.W2 + gamma * self.W2.clamp(min=0)\n",
    "        Vp = self.V + gamma * self.V.clamp(min=0)\n",
    "\n",
    "        # [3.] Define a tensor that will contain the results of the LRP ------------------------------------------------\n",
    "        #      Enable the automatic differentiation mechanism for this tensor ------------------------------------------\n",
    "        X = torch.eye(len(adj_matrix))\n",
    "        X.requires_grad_(True)\n",
    "\n",
    "        # [4.] Multiply the identity matrix (starting values of the LRP results tensor) --------------------------------\n",
    "        #      with the weights of the input layer ---------------------------------------------------------------------\n",
    "        H = X.matmul(self.U).clamp(min=0)  # H_0\n",
    "\n",
    "        ################################################################################################################\n",
    "        # [5.] Perform the computation of equations for the 1st layer as described the original paper ------------------\n",
    "        # [5.1.] (Lambda * H_0) * W_1 = Z_1 * W_1                   # Page 5 of original paper -------------------------\n",
    "        # This is a component of the right part of the equation computing Q_t  -----------------------------------------\n",
    "        # P = (Lambda * H_0) * W_1 = Z_1 * W_1 -------------------------------------------------------------------------\n",
    "        # This variable is useful for the computation of Qt ------------------------------------------------------------\n",
    "        P = adj_matrix.transpose(1, 0).matmul(H.matmul(self.W1))\n",
    "\n",
    "        # [5.2.] Pt = (Lambda * H_0) * W_1^ = Z_1 * W_1^ = P_1           # Page 5 of original paper --------------------\n",
    "        Pt = adj_matrix.transpose(1, 0).matmul(H.matmul(W1p))\n",
    "\n",
    "        # [5.3.] Q_1 = P_1 :::element_wise_mult::: [RelU(Z_1 * W_1) / P_t]cst. -----------------------------------------\n",
    "        Qt = (Pt * (P / (Pt + 1e-6)).data).clamp(min=0)\n",
    "\n",
    "        # [5.4.] H_1 = Q_1 :::element_wise_mult::: M_j + [Q_t]cst. :::element_wise_mult::: (1 - M_j) -------------------\n",
    "        #        Mutliplication with the column array which contains only one non-zero element in the position of the --\n",
    "        #        node index contained in the walk, as defined in step [1.] of this method ------------------------------\n",
    "        if indexes is not None:\n",
    "            H = Qt * M_j + (1 - M_j) * (Qt.data)\n",
    "\n",
    "        ################################################################################################################\n",
    "        # [6.] Perform the computation of equations for the 2nd layer as described the original paper ------------------\n",
    "        #      Corresponds to the equations for the 1st layer ----------------------------------------------------------\n",
    "        P = adj_matrix.transpose(1, 0).matmul(H.matmul(self.W2))\n",
    "        Pt = adj_matrix.transpose(1, 0).matmul(H.matmul(W2p))\n",
    "        Qt = (Pt * (P / (Pt + 1e-6)).data).clamp(min=0)\n",
    "\n",
    "        if indexes is not None:\n",
    "            H = Qt * M_k + (1 - M_k) * (Qt.data)\n",
    "\n",
    "        ################################################################################################################\n",
    "        # [7.] Perform the computation of equations for the 3d layer as described the original paper -------------------\n",
    "        #      Corresponds to the equations for the 1st and 2nd layer --------------------------------------------------\n",
    "        P = H.matmul(self.V)\n",
    "        Pt = H.matmul(Vp)\n",
    "        Qt = (Pt * (P / (Pt + 1e-6)).data).clamp(min=0)\n",
    "\n",
    "        ################################################################################################################\n",
    "        # [8.] Compute the mean w.r.t. the target class and apply the automatic differentiation ------------------------\n",
    "        Y = Qt.mean(dim=0)[target]\n",
    "        Y.backward()\n",
    "\n",
    "        return X.data * X.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# [C.] Train GNN model =================================================================================================\n",
    "########################################################################################################################\n",
    "def train_gnn_model():\n",
    "    \"\"\"\n",
    "    Train the GNN model\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # [1.] Train the model with size of input layer: 10, size of hidden layer: 64, size of output layer: 2 -------------\n",
    "    hidden_layer_size = 64\n",
    "    output_layer_size = 2\n",
    "    gnn_model = GraphNet(nodes_nr, hidden_layer_size, output_layer_size)\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        gnn_model.params, lr=0.001, momentum=0.99\n",
    "    )  # Use SGD optimizer ------------------------\n",
    "    loss_avg = 0.5  # Running average of the MSE ---------------------------------------------------------------------\n",
    "\n",
    "    print(\"Training model:\")\n",
    "    print(\"   iter | loss\")\n",
    "    print(\"   -----------\")\n",
    "\n",
    "    train_iterations = 20001\n",
    "\n",
    "    for iteration in range(0, train_iterations):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # [2.] Create a new scale-free graph ---------------------------------------------------------------------------\n",
    "        input_graph = create_scale_free_graph(\n",
    "            nodes_nr, input_seed=iteration, embed=False\n",
    "        )\n",
    "\n",
    "        # [3.] Forward the input graph's laplacian to the model --------------------------------------------------------\n",
    "        y = gnn_model.forward_pass(input_graph[\"laplacian\"])\n",
    "\n",
    "        # [4.] Compute the error between the true value and the predicted one ------------------------------------------\n",
    "        loss = (y[0] - (input_graph[\"target\"] == 1) * 1.0) ** 2 + (\n",
    "            y[1] - (input_graph[\"target\"] == 2) * 1.0\n",
    "        ) ** 2\n",
    "\n",
    "        #      This is a running average of the MSE --------------------------------------------------------------------\n",
    "        loss_avg = 0.999 * loss_avg + 0.001 * loss.data.numpy()\n",
    "\n",
    "        # [5.] Backpropagate a weighted sum of the average error and the currently computed error ----------------------\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # [6.] Print the average error at each iteration ---------------------------------------------------------------\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"% 8d %.3f\" % (iteration, loss_avg))\n",
    "\n",
    "    return gnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_gnn_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# [D.] Test GNN model ==================================================================================================\n",
    "########################################################################################################################\n",
    "test_size = 200\n",
    "num_false = 0\n",
    "\n",
    "# Create new graphs for the test set -----------------------------------------------------------------------------------\n",
    "for it in range(20001, 20001 + test_size):\n",
    "\n",
    "    test_graph = create_scale_free_graph(nodes_nr, input_seed=it, embed=False)\n",
    "    y = model.forward_pass(test_graph[\"laplacian\"])\n",
    "    prediction = int(y.data.argmax()) + 1\n",
    "\n",
    "    if prediction != test_graph[\"target\"]:\n",
    "        num_false += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"For {} test sample graphs, the model predicts the growth parameter with an accuracy of {} %\".format(\n",
    "        test_size, 100 * (test_size - num_false) / test_size\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# [E.] Explain and plot relevances =====================================================================================\n",
    "########################################################################################################################\n",
    "def explain_graph_LRP(\n",
    "    input_graph: dict, nn: GraphNet, target: int, gamma=None, ax=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Explain a graph prediction through GNN-LRP\n",
    "\n",
    "    :param input_graph: Dictionary of input graph information\n",
    "    :param nn: GNN\n",
    "    :param target: target class\n",
    "    :param gamma: LRP gamma hyperparameter\n",
    "    :param ax: Plot axis\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # [1.] Plot the underlying graph in black --------------------------------------------------------------------------\n",
    "    r = input_graph[\"layout\"]\n",
    "    r = r - r.min(axis=0)\n",
    "    r = r / r.max(axis=0) * 2 - 1\n",
    "\n",
    "    N = len(input_graph[\"adjacency\"])\n",
    "    for i in np.arange(N):\n",
    "        for j in np.arange(N):\n",
    "            if input_graph[\"adjacency\"][i, j] > 0 and i != j:\n",
    "                plt.plot(\n",
    "                    [r[i, 0], r[j, 0]],\n",
    "                    [r[i, 1], r[j, 1]],\n",
    "                    color=\"gray\",\n",
    "                    lw=0.5,\n",
    "                    ls=\"dotted\",\n",
    "                )\n",
    "    ax.plot(r[:, 0], r[:, 1], \"o\", color=\"black\", ms=3)\n",
    "\n",
    "    # [2.] For all walks of length 3 compute and plot the relevances ---------------------------------------------------\n",
    "    for i, j, k in input_graph[\"walks\"]:\n",
    "\n",
    "        R = nn.lrp_computation(input_graph[\"laplacian\"], gamma, target, (j, k))[i].sum()\n",
    "        tx, ty = relevance_curves_xy_computation(\n",
    "            [r[i, 0], r[j, 0], r[k, 0]], [r[i, 1], r[j, 1], r[k, 1]]\n",
    "        )\n",
    "\n",
    "        # Positive (red) or negative (blue) relevance defines the explanation colorbar ----------------------------------------------------------\n",
    "        # Depending on the intensity of the relevance, define the transparency parameter alpha -------------------------\n",
    "        if R > 0.0:\n",
    "            alpha = np.clip(20 * R.data.numpy(), 0, 1)\n",
    "            ax.plot(tx, ty, alpha=alpha, color=\"red\", lw=1.2)\n",
    "\n",
    "        if R < -0.0:\n",
    "            alpha = np.clip(-20 * R.data.numpy(), 0, 1)\n",
    "            ax.plot(tx, ty, alpha=alpha, color=\"blue\", lw=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Generate new graph - new test sample ---------------------------------------------------------------------------------\n",
    "# Pass this example through the GNN and compute the relevance of the paths of this graph -------------------------------\n",
    "#\n",
    "# Red denotes positive relevance/contribution to the predicted class, even if it's a wrong prediction ------------------\n",
    "# Blue denotes negative relevance/contribution to the predicted class, even it it's a wrong prediction -----------------\n",
    "########################################################################################################################\n",
    "gamma = 0.1\n",
    "\n",
    "for target in [0, 1]:\n",
    "\n",
    "    plt.figure(figsize=(3 * len(sample_ids), 3))\n",
    "\n",
    "    for ids, seed in enumerate(sample_ids):\n",
    "        ax = plt.subplot(1, len(sample_ids), ids + 1)\n",
    "        example_graph = create_scale_free_graph(nodes_nr, input_seed=seed, embed=True)\n",
    "\n",
    "        # Explain with LRP ---------------------------------------------------------------------------------------------\n",
    "        explain_graph_LRP(example_graph, model, target, gamma=gamma, ax=ax)\n",
    "        plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.xlim(-1.2, 1.2)\n",
    "        plt.ylim(-1.2, 1.2)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Evidence for growth factor ={target + 1} with $\\gamma={gamma}$ (LRP relevances in red -positive- and blue -negative-)\",\n",
    "        size=14,\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
